{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 03 - Defenses: Dropout & Preprocessing\n",
                "\n",
                "**Goal**: Evaluate simple defense strategies against adversarial attacks.\n",
                "\n",
                "**Defenses studied**:\n",
                "1. **Dropout** - regularization that may improve robustness\n",
                "2. **Gaussian noise preprocessing** - input randomization defense"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Colab setup\n",
                "import sys\n",
                "import os\n",
                "\n",
                "if 'google.colab' in sys.modules:\n",
                "    %cd /content\n",
                "    !git clone https://github.com/cdm34/adversarial-robustness.git 2>/dev/null || true\n",
                "    %cd adversarial-robustness\n",
                "    sys.path.insert(0, '/content/adversarial-robustness')\n",
                "else:\n",
                "    sys.path.insert(0, os.path.abspath('..'))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import matplotlib.pyplot as plt\n",
                "import numpy as np\n",
                "\n",
                "from src import (\n",
                "    FashionMNISTNet, FashionMNISTNetDropout,\n",
                "    DataConfig, get_fashion_mnist_datasets, split_train_val, make_loaders,\n",
                "    TrainConfig, fit,\n",
                "    AttackConfig, fgsm, pgd_linf,\n",
                "    accuracy, add_gaussian_noise,\n",
                "    get_device, set_seed,\n",
                "    plot_robustness_comparison, save_figure,\n",
                "    FASHION_MNIST_CLASSES,\n",
                ")\n",
                "\n",
                "print(f\"PyTorch version: {torch.__version__}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "set_seed(42)\n",
                "device = get_device()\n",
                "print(f\"Using device: {device}\")\n",
                "\n",
                "# Load data\n",
                "train_ds, test_ds = get_fashion_mnist_datasets()\n",
                "data_cfg = DataConfig(batch_size=128, val_ratio=0.1)\n",
                "train_subset, val_subset = split_train_val(train_ds, data_cfg.val_ratio, data_cfg.seed)\n",
                "train_loader, val_loader, test_loader = make_loaders(\n",
                "    train_subset, val_subset, test_ds, data_cfg, device\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Train Models"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "train_cfg = TrainConfig(epochs=10, lr=1e-3)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Try to load baseline, otherwise train\n",
                "baseline_model = FashionMNISTNet().to(device)\n",
                "\n",
                "if os.path.exists('checkpoints/baseline_cnn.pt'):\n",
                "    checkpoint = torch.load('checkpoints/baseline_cnn.pt', map_location=device)\n",
                "    baseline_model.load_state_dict(checkpoint['model_state_dict'])\n",
                "    print(\"Loaded baseline model\")\n",
                "else:\n",
                "    print(\"Training baseline model...\")\n",
                "    fit(baseline_model, train_loader, val_loader, device, train_cfg)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train dropout model\n",
                "dropout_model = FashionMNISTNetDropout(p=0.3).to(device)\n",
                "\n",
                "if os.path.exists('checkpoints/dropout_cnn.pt'):\n",
                "    checkpoint = torch.load('checkpoints/dropout_cnn.pt', map_location=device)\n",
                "    dropout_model.load_state_dict(checkpoint['model_state_dict'])\n",
                "    print(\"Loaded dropout model\")\n",
                "else:\n",
                "    print(\"Training dropout model (p=0.3)...\")\n",
                "    result = fit(dropout_model, train_loader, val_loader, device, train_cfg)\n",
                "    os.makedirs('checkpoints', exist_ok=True)\n",
                "    torch.save({'model_state_dict': dropout_model.state_dict()}, 'checkpoints/dropout_cnn.pt')\n",
                "    print(f\"Best val acc: {result['best_val_acc']:.2f}%\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Evaluate Clean Accuracy"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "baseline_clean = accuracy(baseline_model, test_loader, device)\n",
                "dropout_clean = accuracy(dropout_model, test_loader, device)\n",
                "\n",
                "print(\"Clean Test Accuracy:\")\n",
                "print(f\"  Baseline:  {baseline_clean:.2f}%\")\n",
                "print(f\"  Dropout:   {dropout_clean:.2f}%\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Evaluate Robustness"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def evaluate_robust_accuracy(model, loader, attack_fn, attack_cfg, device):\n",
                "    \"\"\"Compute accuracy on adversarial examples.\"\"\"\n",
                "    model.eval()\n",
                "    correct = 0\n",
                "    total = 0\n",
                "    \n",
                "    for x, y in loader:\n",
                "        x, y = x.to(device), y.to(device)\n",
                "        x_adv = attack_fn(model, x, y, attack_cfg)\n",
                "        \n",
                "        with torch.no_grad():\n",
                "            preds = model(x_adv).argmax(dim=1)\n",
                "            correct += (preds == y).sum().item()\n",
                "            total += y.size(0)\n",
                "    \n",
                "    return 100.0 * correct / total"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# FGSM attack\n",
                "eps = 0.1\n",
                "fgsm_cfg = AttackConfig(eps=eps)\n",
                "\n",
                "baseline_fgsm = evaluate_robust_accuracy(baseline_model, test_loader, fgsm, fgsm_cfg, device)\n",
                "dropout_fgsm = evaluate_robust_accuracy(dropout_model, test_loader, fgsm, fgsm_cfg, device)\n",
                "\n",
                "print(f\"FGSM Robustness (ε={eps}):\")\n",
                "print(f\"  Baseline:  {baseline_fgsm:.2f}%\")\n",
                "print(f\"  Dropout:   {dropout_fgsm:.2f}%\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# PGD attack\n",
                "pgd_cfg = AttackConfig(eps=eps, steps=10, step_size=eps/4)\n",
                "\n",
                "baseline_pgd = evaluate_robust_accuracy(baseline_model, test_loader, pgd_linf, pgd_cfg, device)\n",
                "dropout_pgd = evaluate_robust_accuracy(dropout_model, test_loader, pgd_linf, pgd_cfg, device)\n",
                "\n",
                "print(f\"PGD Robustness (ε={eps}):\")\n",
                "print(f\"  Baseline:  {baseline_pgd:.2f}%\")\n",
                "print(f\"  Dropout:   {dropout_pgd:.2f}%\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Gaussian Noise Defense"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def evaluate_with_noise_defense(model, loader, attack_fn, attack_cfg, noise_sigma, device):\n",
                "    \"\"\"Evaluate with Gaussian noise preprocessing as defense.\"\"\"\n",
                "    model.eval()\n",
                "    correct = 0\n",
                "    total = 0\n",
                "    \n",
                "    for x, y in loader:\n",
                "        x, y = x.to(device), y.to(device)\n",
                "        \n",
                "        # Generate adversarial on original input\n",
                "        x_adv = attack_fn(model, x, y, attack_cfg)\n",
                "        \n",
                "        # Apply noise preprocessing\n",
                "        x_defended = add_gaussian_noise(x_adv, sigma=noise_sigma)\n",
                "        \n",
                "        with torch.no_grad():\n",
                "            preds = model(x_defended).argmax(dim=1)\n",
                "            correct += (preds == y).sum().item()\n",
                "            total += y.size(0)\n",
                "    \n",
                "    return 100.0 * correct / total"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test different noise levels\n",
                "noise_sigmas = [0.0, 0.02, 0.05, 0.1, 0.15]\n",
                "\n",
                "print(f\"Gaussian Noise Defense vs FGSM (ε={eps}):\")\n",
                "print(\"-\" * 50)\n",
                "\n",
                "noise_results = []\n",
                "for sigma in noise_sigmas:\n",
                "    if sigma == 0.0:\n",
                "        acc = baseline_fgsm\n",
                "    else:\n",
                "        acc = evaluate_with_noise_defense(baseline_model, test_loader, fgsm, fgsm_cfg, sigma, device)\n",
                "    noise_results.append(acc)\n",
                "    print(f\"  σ = {sigma:.2f}: {acc:.2f}%\")\n",
                "\n",
                "# Also test on clean data to see trade-off\n",
                "print(\"\\nClean accuracy with noise preprocessing:\")\n",
                "for sigma in noise_sigmas:\n",
                "    if sigma == 0.0:\n",
                "        acc = baseline_clean\n",
                "    else:\n",
                "        # No attack, just noise\n",
                "        correct = 0\n",
                "        total = 0\n",
                "        for x, y in test_loader:\n",
                "            x, y = x.to(device), y.to(device)\n",
                "            x_noisy = add_gaussian_noise(x, sigma=sigma)\n",
                "            with torch.no_grad():\n",
                "                preds = baseline_model(x_noisy).argmax(dim=1)\n",
                "                correct += (preds == y).sum().item()\n",
                "                total += y.size(0)\n",
                "        acc = 100.0 * correct / total\n",
                "    print(f\"  σ = {sigma:.2f}: {acc:.2f}%\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Comparison Summary"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Summary table\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"DEFENSE COMPARISON SUMMARY\")\n",
                "print(\"=\"*60)\n",
                "print(f\"{'Defense':<20} {'Clean Acc':>12} {'FGSM Acc':>12} {'PGD Acc':>12}\")\n",
                "print(\"-\"*60)\n",
                "print(f\"{'Baseline':<20} {baseline_clean:>11.2f}% {baseline_fgsm:>11.2f}% {baseline_pgd:>11.2f}%\")\n",
                "print(f\"{'Dropout (p=0.3)':<20} {dropout_clean:>11.2f}% {dropout_fgsm:>11.2f}% {dropout_pgd:>11.2f}%\")\n",
                "print(\"=\"*60)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Bar chart comparison\n",
                "fig = plot_robustness_comparison(\n",
                "    model_names=['Baseline', 'Dropout (p=0.3)'],\n",
                "    clean_accs=[baseline_clean, dropout_clean],\n",
                "    robust_accs=[baseline_fgsm, dropout_fgsm],\n",
                "    attack_name=f'FGSM (ε={eps})'\n",
                ")\n",
                "save_figure(fig, 'defense_comparison_fgsm')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# PGD comparison\n",
                "fig = plot_robustness_comparison(\n",
                "    model_names=['Baseline', 'Dropout (p=0.3)'],\n",
                "    clean_accs=[baseline_clean, dropout_clean],\n",
                "    robust_accs=[baseline_pgd, dropout_pgd],\n",
                "    attack_name=f'PGD (ε={eps})'\n",
                ")\n",
                "save_figure(fig, 'defense_comparison_pgd')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Summary\n",
                "\n",
                "**Key Findings**:\n",
                "1. **Dropout** provides marginal robustness improvement but not significant\n",
                "2. **Gaussian noise preprocessing** can slightly improve robustness but degrades clean accuracy\n",
                "3. These simple defenses are **not sufficient** for robust models\n",
                "\n",
                "**Trade-offs**:\n",
                "- Clean accuracy vs. robustness is a key tension\n",
                "- Input randomization has diminishing returns\n",
                "\n",
                "**Next**: Adversarial training for principled robustness"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}