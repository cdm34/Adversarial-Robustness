{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 02 - FGSM & PGD Adversarial Attacks\n",
                "\n",
                "**Goal**: Evaluate model vulnerability to adversarial attacks.\n",
                "\n",
                "**Research Questions**:\n",
                "- How does adversarial noise affect CNN accuracy?\n",
                "- How does model confidence change on adversarial examples?\n",
                "- Is PGD more effective than FGSM?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Colab setup\n",
                "import sys\n",
                "import os\n",
                "\n",
                "if 'google.colab' in sys.modules:\n",
                "    %cd /content\n",
                "    !git clone https://github.com/cdm34/adversarial-robustness.git 2>/dev/null || true\n",
                "    %cd adversarial-robustness\n",
                "    sys.path.insert(0, '/content/adversarial-robustness')\n",
                "else:\n",
                "    sys.path.insert(0, os.path.abspath('..'))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import matplotlib.pyplot as plt\n",
                "import numpy as np\n",
                "\n",
                "from src import (\n",
                "    FashionMNISTNet,\n",
                "    DataConfig, get_fashion_mnist_datasets, split_train_val, make_loaders,\n",
                "    AttackConfig, fgsm, pgd_linf,\n",
                "    accuracy, confidence_stats, attack_success_rate,\n",
                "    get_device, set_seed,\n",
                "    plot_adversarial_examples, plot_epsilon_vs_accuracy, save_figure,\n",
                "    FASHION_MNIST_CLASSES,\n",
                ")\n",
                "\n",
                "print(f\"PyTorch version: {torch.__version__}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup & Load Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "set_seed(42)\n",
                "device = get_device()\n",
                "print(f\"Using device: {device}\")\n",
                "\n",
                "# Load data\n",
                "train_ds, test_ds = get_fashion_mnist_datasets()\n",
                "data_cfg = DataConfig(batch_size=128, val_ratio=0.0)  # No validation needed here\n",
                "train_subset, val_subset = split_train_val(train_ds, data_cfg.val_ratio)\n",
                "train_loader, val_loader, test_loader = make_loaders(\n",
                "    train_subset, val_subset, test_ds, data_cfg, device\n",
                ")\n",
                "\n",
                "print(f\"Test batches: {len(test_loader)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load trained baseline model\n",
                "model = FashionMNISTNet().to(device)\n",
                "\n",
                "checkpoint_path = 'checkpoints/baseline_cnn.pt'\n",
                "if os.path.exists(checkpoint_path):\n",
                "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
                "    model.load_state_dict(checkpoint['model_state_dict'])\n",
                "    print(f\"Loaded model with test accuracy: {checkpoint['test_accuracy']:.2f}%\")\n",
                "else:\n",
                "    print(\"WARNING: No checkpoint found. Training a quick model...\")\n",
                "    from src import TrainConfig, fit\n",
                "    train_ds, test_ds = get_fashion_mnist_datasets()\n",
                "    train_subset, val_subset = split_train_val(train_ds, 0.1)\n",
                "    train_loader, val_loader, test_loader = make_loaders(\n",
                "        train_subset, val_subset, test_ds, DataConfig(batch_size=128), device\n",
                "    )\n",
                "    fit(model, train_loader, val_loader, device, TrainConfig(epochs=5))\n",
                "\n",
                "model.eval()\n",
                "clean_acc = accuracy(model, test_loader, device)\n",
                "print(f\"Clean accuracy: {clean_acc:.2f}%\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. FGSM Attack Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test FGSM at various epsilon values\n",
                "epsilons = [0.0, 0.01, 0.03, 0.05, 0.1, 0.15, 0.2, 0.3]\n",
                "fgsm_accuracies = []\n",
                "\n",
                "print(\"FGSM Attack Results:\")\n",
                "print(\"-\" * 40)\n",
                "\n",
                "for eps in epsilons:\n",
                "    if eps == 0.0:\n",
                "        acc = clean_acc\n",
                "    else:\n",
                "        cfg = AttackConfig(eps=eps)\n",
                "        correct = 0\n",
                "        total = 0\n",
                "        \n",
                "        for x, y in test_loader:\n",
                "            x, y = x.to(device), y.to(device)\n",
                "            x_adv = fgsm(model, x, y, cfg)\n",
                "            \n",
                "            with torch.no_grad():\n",
                "                preds = model(x_adv).argmax(dim=1)\n",
                "                correct += (preds == y).sum().item()\n",
                "                total += y.size(0)\n",
                "        \n",
                "        acc = 100.0 * correct / total\n",
                "    \n",
                "    fgsm_accuracies.append(acc)\n",
                "    print(f\"  ε = {eps:.2f}: {acc:.2f}%\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. PGD Attack Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test PGD at various epsilon values\n",
                "pgd_accuracies = []\n",
                "\n",
                "print(\"PGD Attack Results (10 steps):\")\n",
                "print(\"-\" * 40)\n",
                "\n",
                "for eps in epsilons:\n",
                "    if eps == 0.0:\n",
                "        acc = clean_acc\n",
                "    else:\n",
                "        cfg = AttackConfig(eps=eps, steps=10, step_size=eps/4)\n",
                "        correct = 0\n",
                "        total = 0\n",
                "        \n",
                "        for x, y in test_loader:\n",
                "            x, y = x.to(device), y.to(device)\n",
                "            x_adv = pgd_linf(model, x, y, cfg)\n",
                "            \n",
                "            with torch.no_grad():\n",
                "                preds = model(x_adv).argmax(dim=1)\n",
                "                correct += (preds == y).sum().item()\n",
                "                total += y.size(0)\n",
                "        \n",
                "        acc = 100.0 * correct / total\n",
                "    \n",
                "    pgd_accuracies.append(acc)\n",
                "    print(f\"  ε = {eps:.2f}: {acc:.2f}%\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot accuracy vs epsilon\n",
                "fig = plot_epsilon_vs_accuracy(epsilons, clean_acc, fgsm_accuracies, pgd_accuracies)\n",
                "save_figure(fig, 'accuracy_vs_epsilon')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Visualize Adversarial Examples"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get a batch for visualization\n",
                "x_batch, y_batch = next(iter(test_loader))\n",
                "x_batch, y_batch = x_batch[:10].to(device), y_batch[:10].to(device)\n",
                "\n",
                "# Generate adversarial examples with FGSM\n",
                "eps_vis = 0.1\n",
                "cfg_vis = AttackConfig(eps=eps_vis)\n",
                "x_adv = fgsm(model, x_batch, y_batch, cfg_vis)\n",
                "\n",
                "# Get predictions\n",
                "with torch.no_grad():\n",
                "    preds_clean = model(x_batch).argmax(dim=1)\n",
                "    preds_adv = model(x_adv).argmax(dim=1)\n",
                "\n",
                "# Visualize\n",
                "fig = plot_adversarial_examples(\n",
                "    x_batch, x_adv, y_batch, preds_clean, preds_adv,\n",
                "    num_samples=5, eps=eps_vis\n",
                ")\n",
                "save_figure(fig, 'fgsm_adversarial_examples')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# PGD adversarial examples\n",
                "cfg_pgd = AttackConfig(eps=eps_vis, steps=10, step_size=eps_vis/4)\n",
                "x_adv_pgd = pgd_linf(model, x_batch, y_batch, cfg_pgd)\n",
                "\n",
                "with torch.no_grad():\n",
                "    preds_adv_pgd = model(x_adv_pgd).argmax(dim=1)\n",
                "\n",
                "fig = plot_adversarial_examples(\n",
                "    x_batch, x_adv_pgd, y_batch, preds_clean, preds_adv_pgd,\n",
                "    num_samples=5, eps=eps_vis\n",
                ")\n",
                "fig.suptitle('PGD Adversarial Examples', fontsize=12)\n",
                "save_figure(fig, 'pgd_adversarial_examples')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Confidence Analysis (AI Safety)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Compare confidence on clean vs adversarial examples\n",
                "eps_test = 0.1\n",
                "cfg_test = AttackConfig(eps=eps_test)\n",
                "\n",
                "clean_confs = []\n",
                "adv_confs = []\n",
                "\n",
                "for x, y in test_loader:\n",
                "    x, y = x.to(device), y.to(device)\n",
                "    x_adv = fgsm(model, x, y, cfg_test)\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        clean_probs = torch.softmax(model(x), dim=1)\n",
                "        adv_probs = torch.softmax(model(x_adv), dim=1)\n",
                "        \n",
                "        clean_confs.extend(clean_probs.max(dim=1)[0].cpu().tolist())\n",
                "        adv_confs.extend(adv_probs.max(dim=1)[0].cpu().tolist())\n",
                "\n",
                "print(f\"Clean examples - Mean confidence: {np.mean(clean_confs):.3f}\")\n",
                "print(f\"Adversarial (ε={eps_test}) - Mean confidence: {np.mean(adv_confs):.3f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Confidence distribution comparison\n",
                "fig, ax = plt.subplots(figsize=(10, 5))\n",
                "\n",
                "ax.hist(clean_confs, bins=50, alpha=0.6, label='Clean', color='blue', density=True)\n",
                "ax.hist(adv_confs, bins=50, alpha=0.6, label=f'Adversarial (ε={eps_test})', color='red', density=True)\n",
                "\n",
                "ax.axvline(np.mean(clean_confs), color='blue', linestyle='--', linewidth=2)\n",
                "ax.axvline(np.mean(adv_confs), color='red', linestyle='--', linewidth=2)\n",
                "\n",
                "ax.set_xlabel('Confidence (max softmax probability)', fontsize=12)\n",
                "ax.set_ylabel('Density', fontsize=12)\n",
                "ax.set_title('Model Confidence: Clean vs. Adversarial Examples', fontsize=14)\n",
                "ax.legend()\n",
                "ax.grid(True, alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "save_figure(fig, 'confidence_distribution')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Attack success rate\n",
                "print(\"\\nAttack Success Rates:\")\n",
                "print(\"-\" * 40)\n",
                "\n",
                "for eps in [0.05, 0.1, 0.15, 0.2]:\n",
                "    cfg = AttackConfig(eps=eps)\n",
                "    \n",
                "    fgsm_result = attack_success_rate(model, test_loader, fgsm, cfg, device)\n",
                "    pgd_result = attack_success_rate(model, test_loader, pgd_linf, \n",
                "                                      AttackConfig(eps=eps, steps=10, step_size=eps/4), device)\n",
                "    \n",
                "    print(f\"ε = {eps:.2f}: FGSM = {fgsm_result['attack_success_rate']:.1f}%, \"\n",
                "          f\"PGD = {pgd_result['attack_success_rate']:.1f}%\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Summary\n",
                "\n",
                "**Key Findings**:\n",
                "1. Model accuracy degrades rapidly with increasing ε\n",
                "2. PGD attacks are stronger than FGSM (as expected)\n",
                "3. Model remains overconfident even on misclassified adversarial examples ⚠️\n",
                "4. At ε=0.1, attack success rate is TBD\n",
                "\n",
                "**AI Safety Implications**:\n",
                "- High confidence on wrong predictions is dangerous in safety-critical applications\n",
                "- Standard training provides no adversarial robustness\n",
                "\n",
                "**Next**: Evaluate defenses (dropout, preprocessing, adversarial training)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
