{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 04 - Adversarial Training\n",
                "\n",
                "**Goal**: Train models on adversarial examples to improve robustness.\n",
                "\n",
                "**Research Questions**:\n",
                "- How much robustness does adversarial training provide?\n",
                "- What is the clean accuracy trade-off?\n",
                "- Is PGD adversarial training stronger than FGSM?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Colab setup\n",
                "import sys\n",
                "import os\n",
                "\n",
                "if 'google.colab' in sys.modules:\n",
                "    %cd /content\n",
                "    !git clone https://github.com/cdm34/adversarial-robustness.git 2>/dev/null || true\n",
                "    %cd adversarial-robustness\n",
                "    sys.path.insert(0, '/content/adversarial-robustness')\n",
                "else:\n",
                "    sys.path.insert(0, os.path.abspath('..'))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import matplotlib.pyplot as plt\n",
                "import numpy as np\n",
                "\n",
                "from src import (\n",
                "    FashionMNISTNet,\n",
                "    DataConfig, get_fashion_mnist_datasets, split_train_val, make_loaders,\n",
                "    AttackConfig, fgsm, pgd_linf,\n",
                "    AdvTrainConfig, adversarial_train_fgsm, adversarial_train_pgd, mixed_adversarial_training,\n",
                "    accuracy, confidence_stats,\n",
                "    get_device, set_seed,\n",
                "    plot_training_curves, plot_robustness_comparison, plot_epsilon_vs_accuracy, save_figure,\n",
                "    FASHION_MNIST_CLASSES,\n",
                ")\n",
                "\n",
                "print(f\"PyTorch version: {torch.__version__}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "set_seed(42)\n",
                "device = get_device()\n",
                "print(f\"Using device: {device}\")\n",
                "\n",
                "# Load data\n",
                "train_ds, test_ds = get_fashion_mnist_datasets()\n",
                "data_cfg = DataConfig(batch_size=128, val_ratio=0.1)\n",
                "train_subset, val_subset = split_train_val(train_ds, data_cfg.val_ratio, data_cfg.seed)\n",
                "train_loader, val_loader, test_loader = make_loaders(\n",
                "    train_subset, val_subset, test_ds, data_cfg, device\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Load Baseline Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from src import TrainConfig, fit\n",
                "\n",
                "baseline_model = FashionMNISTNet().to(device)\n",
                "\n",
                "if os.path.exists('checkpoints/baseline_cnn.pt'):\n",
                "    checkpoint = torch.load('checkpoints/baseline_cnn.pt', map_location=device)\n",
                "    baseline_model.load_state_dict(checkpoint['model_state_dict'])\n",
                "    print(\"Loaded baseline model\")\n",
                "else:\n",
                "    print(\"Training baseline model...\")\n",
                "    fit(baseline_model, train_loader, val_loader, device, TrainConfig(epochs=10))\n",
                "\n",
                "baseline_clean = accuracy(baseline_model, test_loader, device)\n",
                "print(f\"Baseline clean accuracy: {baseline_clean:.2f}%\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. FGSM Adversarial Training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train with FGSM adversarial examples\n",
                "fgsm_adv_model = FashionMNISTNet().to(device)\n",
                "\n",
                "adv_cfg = AdvTrainConfig(epochs=10, lr=1e-3, attack_eps=0.1)\n",
                "\n",
                "if os.path.exists('checkpoints/fgsm_adv_trained.pt'):\n",
                "    checkpoint = torch.load('checkpoints/fgsm_adv_trained.pt', map_location=device)\n",
                "    fgsm_adv_model.load_state_dict(checkpoint['model_state_dict'])\n",
                "    print(\"Loaded FGSM adversarially trained model\")\n",
                "else:\n",
                "    print(\"Training with FGSM adversarial examples (ε=0.1)...\")\n",
                "    print(\"This may take a few minutes...\")\n",
                "    fgsm_result = adversarial_train_fgsm(fgsm_adv_model, train_loader, val_loader, device, adv_cfg)\n",
                "    \n",
                "    os.makedirs('checkpoints', exist_ok=True)\n",
                "    torch.save({'model_state_dict': fgsm_adv_model.state_dict()}, 'checkpoints/fgsm_adv_trained.pt')\n",
                "    print(f\"Best val acc: {fgsm_result['best_val_acc']:.2f}%\")\n",
                "    \n",
                "    # Plot training curves\n",
                "    fig = plot_training_curves(fgsm_result['history'], title='FGSM Adversarial Training')\n",
                "    save_figure(fig, 'fgsm_adv_training_curves')\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. PGD Adversarial Training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train with PGD adversarial examples (stronger)\n",
                "pgd_adv_model = FashionMNISTNet().to(device)\n",
                "\n",
                "if os.path.exists('checkpoints/pgd_adv_trained.pt'):\n",
                "    checkpoint = torch.load('checkpoints/pgd_adv_trained.pt', map_location=device)\n",
                "    pgd_adv_model.load_state_dict(checkpoint['model_state_dict'])\n",
                "    print(\"Loaded PGD adversarially trained model\")\n",
                "else:\n",
                "    print(\"Training with PGD adversarial examples (ε=0.1)...\")\n",
                "    print(\"This will take longer than FGSM training...\")\n",
                "    pgd_result = adversarial_train_pgd(pgd_adv_model, train_loader, val_loader, device, adv_cfg)\n",
                "    \n",
                "    torch.save({'model_state_dict': pgd_adv_model.state_dict()}, 'checkpoints/pgd_adv_trained.pt')\n",
                "    print(f\"Best val acc: {pgd_result['best_val_acc']:.2f}%\")\n",
                "    \n",
                "    fig = plot_training_curves(pgd_result['history'], title='PGD Adversarial Training')\n",
                "    save_figure(fig, 'pgd_adv_training_curves')\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Mixed Adversarial Training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train with 50% clean, 50% adversarial (better clean accuracy)\n",
                "mixed_model = FashionMNISTNet().to(device)\n",
                "\n",
                "if os.path.exists('checkpoints/mixed_adv_trained.pt'):\n",
                "    checkpoint = torch.load('checkpoints/mixed_adv_trained.pt', map_location=device)\n",
                "    mixed_model.load_state_dict(checkpoint['model_state_dict'])\n",
                "    print(\"Loaded mixed adversarially trained model\")\n",
                "else:\n",
                "    print(\"Training with mixed adversarial examples (50/50)...\")\n",
                "    mixed_result = mixed_adversarial_training(\n",
                "        mixed_model, train_loader, val_loader, device, adv_cfg, mix_ratio=0.5\n",
                "    )\n",
                "    \n",
                "    torch.save({'model_state_dict': mixed_model.state_dict()}, 'checkpoints/mixed_adv_trained.pt')\n",
                "    print(f\"Best val acc: {mixed_result['best_val_acc']:.2f}%\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Evaluate All Models"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def evaluate_robust(model, loader, eps, device):\n",
                "    \"\"\"Evaluate robustness against FGSM and PGD.\"\"\"\n",
                "    fgsm_cfg = AttackConfig(eps=eps)\n",
                "    pgd_cfg = AttackConfig(eps=eps, steps=10, step_size=eps/4)\n",
                "    \n",
                "    fgsm_correct = 0\n",
                "    pgd_correct = 0\n",
                "    total = 0\n",
                "    \n",
                "    for x, y in loader:\n",
                "        x, y = x.to(device), y.to(device)\n",
                "        \n",
                "        x_fgsm = fgsm(model, x, y, fgsm_cfg)\n",
                "        x_pgd = pgd_linf(model, x, y, pgd_cfg)\n",
                "        \n",
                "        with torch.no_grad():\n",
                "            fgsm_correct += (model(x_fgsm).argmax(1) == y).sum().item()\n",
                "            pgd_correct += (model(x_pgd).argmax(1) == y).sum().item()\n",
                "            total += y.size(0)\n",
                "    \n",
                "    return 100.0 * fgsm_correct / total, 100.0 * pgd_correct / total"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Evaluate all models\n",
                "eps = 0.1\n",
                "\n",
                "models = {\n",
                "    'Baseline': baseline_model,\n",
                "    'FGSM Adv Train': fgsm_adv_model,\n",
                "    'PGD Adv Train': pgd_adv_model,\n",
                "    'Mixed (50/50)': mixed_model,\n",
                "}\n",
                "\n",
                "results = {}\n",
                "print(f\"\\nEvaluation (ε={eps}):\")\n",
                "print(\"=\"*70)\n",
                "print(f\"{'Model':<18} {'Clean Acc':>12} {'FGSM Robust':>12} {'PGD Robust':>12}\")\n",
                "print(\"-\"*70)\n",
                "\n",
                "for name, model in models.items():\n",
                "    clean_acc = accuracy(model, test_loader, device)\n",
                "    fgsm_acc, pgd_acc = evaluate_robust(model, test_loader, eps, device)\n",
                "    \n",
                "    results[name] = {'clean': clean_acc, 'fgsm': fgsm_acc, 'pgd': pgd_acc}\n",
                "    print(f\"{name:<18} {clean_acc:>11.2f}% {fgsm_acc:>11.2f}% {pgd_acc:>11.2f}%\")\n",
                "\n",
                "print(\"=\"*70)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Bar chart comparison\n",
                "model_names = list(results.keys())\n",
                "clean_accs = [results[n]['clean'] for n in model_names]\n",
                "fgsm_accs = [results[n]['fgsm'] for n in model_names]\n",
                "pgd_accs = [results[n]['pgd'] for n in model_names]\n",
                "\n",
                "fig, ax = plt.subplots(figsize=(12, 6))\n",
                "\n",
                "x = np.arange(len(model_names))\n",
                "width = 0.25\n",
                "\n",
                "bars1 = ax.bar(x - width, clean_accs, width, label='Clean', color='steelblue')\n",
                "bars2 = ax.bar(x, fgsm_accs, width, label='FGSM', color='coral')\n",
                "bars3 = ax.bar(x + width, pgd_accs, width, label='PGD', color='seagreen')\n",
                "\n",
                "ax.set_xlabel('Model', fontsize=12)\n",
                "ax.set_ylabel('Accuracy (%)', fontsize=12)\n",
                "ax.set_title(f'Adversarial Training Comparison (ε={eps})', fontsize=14)\n",
                "ax.set_xticks(x)\n",
                "ax.set_xticklabels(model_names, rotation=15, ha='right')\n",
                "ax.legend(loc='best')\n",
                "ax.set_ylim(0, 100)\n",
                "ax.grid(True, alpha=0.3, axis='y')\n",
                "\n",
                "# Add value labels\n",
                "for bars in [bars1, bars2, bars3]:\n",
                "    for bar in bars:\n",
                "        height = bar.get_height()\n",
                "        ax.annotate(f'{height:.1f}%', xy=(bar.get_x() + bar.get_width()/2, height),\n",
                "                    xytext=(0, 3), textcoords='offset points', ha='center', va='bottom', fontsize=8)\n",
                "\n",
                "plt.tight_layout()\n",
                "save_figure(fig, 'adversarial_training_comparison')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Robustness vs. Attack Strength"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test at various epsilon values\n",
                "epsilons = [0.0, 0.01, 0.03, 0.05, 0.1, 0.15, 0.2]\n",
                "\n",
                "pgd_trained_fgsm = []\n",
                "pgd_trained_pgd = []\n",
                "baseline_fgsm = []\n",
                "baseline_pgd = []\n",
                "\n",
                "print(\"Computing robustness curves...\")\n",
                "for eps in epsilons:\n",
                "    if eps == 0:\n",
                "        base_clean = accuracy(baseline_model, test_loader, device)\n",
                "        pgd_clean = accuracy(pgd_adv_model, test_loader, device)\n",
                "        baseline_fgsm.append(base_clean)\n",
                "        baseline_pgd.append(base_clean)\n",
                "        pgd_trained_fgsm.append(pgd_clean)\n",
                "        pgd_trained_pgd.append(pgd_clean)\n",
                "    else:\n",
                "        bf, bp = evaluate_robust(baseline_model, test_loader, eps, device)\n",
                "        pf, pp = evaluate_robust(pgd_adv_model, test_loader, eps, device)\n",
                "        baseline_fgsm.append(bf)\n",
                "        baseline_pgd.append(bp)\n",
                "        pgd_trained_fgsm.append(pf)\n",
                "        pgd_trained_pgd.append(pp)\n",
                "    print(f\"  ε={eps:.2f} done\")\n",
                "\n",
                "print(\"Done!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot robustness curves\n",
                "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "# FGSM comparison\n",
                "ax1.plot(epsilons, baseline_fgsm, 'b-o', label='Baseline', markersize=6)\n",
                "ax1.plot(epsilons, pgd_trained_fgsm, 'g-s', label='PGD Adv Trained', markersize=6)\n",
                "ax1.set_xlabel('Perturbation (ε)', fontsize=12)\n",
                "ax1.set_ylabel('Accuracy (%)', fontsize=12)\n",
                "ax1.set_title('FGSM Attack', fontsize=14)\n",
                "ax1.legend(loc='best')\n",
                "ax1.grid(True, alpha=0.3)\n",
                "ax1.set_ylim(0, 100)\n",
                "\n",
                "# PGD comparison\n",
                "ax2.plot(epsilons, baseline_pgd, 'b-o', label='Baseline', markersize=6)\n",
                "ax2.plot(epsilons, pgd_trained_pgd, 'g-s', label='PGD Adv Trained', markersize=6)\n",
                "ax2.set_xlabel('Perturbation (ε)', fontsize=12)\n",
                "ax2.set_ylabel('Accuracy (%)', fontsize=12)\n",
                "ax2.set_title('PGD Attack', fontsize=14)\n",
                "ax2.legend(loc='best')\n",
                "ax2.grid(True, alpha=0.3)\n",
                "ax2.set_ylim(0, 100)\n",
                "\n",
                "fig.suptitle('Robustness: Baseline vs. PGD Adversarial Training', fontsize=14)\n",
                "plt.tight_layout()\n",
                "save_figure(fig, 'robustness_curves_comparison')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Trade-off Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Clean vs. Robust accuracy trade-off\n",
                "fig, ax = plt.subplots(figsize=(8, 6))\n",
                "\n",
                "for name in results:\n",
                "    ax.scatter(results[name]['clean'], results[name]['pgd'], s=150, label=name)\n",
                "    ax.annotate(name, (results[name]['clean'], results[name]['pgd']), \n",
                "                textcoords='offset points', xytext=(5, 5), fontsize=9)\n",
                "\n",
                "ax.set_xlabel('Clean Accuracy (%)', fontsize=12)\n",
                "ax.set_ylabel('PGD Robust Accuracy (%)', fontsize=12)\n",
                "ax.set_title('Clean vs. Robust Accuracy Trade-off', fontsize=14)\n",
                "ax.grid(True, alpha=0.3)\n",
                "\n",
                "# Ideal point would be top-right\n",
                "ax.plot([100], [100], 'r*', markersize=20, label='Ideal')\n",
                "\n",
                "ax.set_xlim(0, 100)\n",
                "ax.set_ylim(0, 100)\n",
                "\n",
                "plt.tight_layout()\n",
                "save_figure(fig, 'clean_vs_robust_tradeoff')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Summary\n",
                "\n",
                "**Key Findings**:\n",
                "\n",
                "1. **Adversarial training significantly improves robustness**\n",
                "   - Baseline: ~20% robust accuracy vs. PGD trained: ~50-60%+\n",
                "\n",
                "2. **PGD training is stronger than FGSM training**\n",
                "   - PGD-trained models are more robust to both FGSM and PGD attacks\n",
                "\n",
                "3. **Clean accuracy trade-off exists**\n",
                "   - Adversarially trained models lose ~5-10% clean accuracy\n",
                "   - Mixed training helps reduce this gap\n",
                "\n",
                "4. **No free lunch in adversarial robustness**\n",
                "   - More robust models require training-time computation\n",
                "   - Robustness at ε often doesn't generalize to larger ε\n",
                "\n",
                "**AI Safety Implications**:\n",
                "- Standard models are highly vulnerable\n",
                "- Adversarial training is essential for deployable robust models\n",
                "- Trade-offs must be carefully considered for safety-critical applications"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}